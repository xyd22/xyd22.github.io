---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# üí° About Me

Hello and welcome! I am Yudong Xie, an undergraduate student in <a href="https://www.ee.tsinghua.edu.cn/en/" style="text-decoration: none;">Department of Electronic Engineering</a>, <a href="https://www.tsinghua.edu.cn/en/" style="text-decoration: none;">Tsinghua University</a>.

My research interests include **human-machine interface** and **wearable electronic devices**. Specifically, I am interested in designing integrated sensing systems for both biomedical applications and intellectual interactions with machines. By integrating advanced technologies, such as micro-electromechanical systems (MEMS), microfabrication, and machine learning, I aspire to explore new possibilities that can change human life, healthcare, and entertainment.

Starting from my sophomore year, I have worked under the supervision of <a href="https://www.sic.tsinghua.edu.cn/en/info/1078/1373.htm" style="text-decoration: none;">Prof. Tianling Ren</a> at <a href="https://www.sic.tsinghua.edu.cn/en/" style="text-decoration: none;">School of Integrated Circuit</a>, <a href="https://www.tsinghua.edu.cn/en/" style="text-decoration: none;">Tsinghua University</a>. My research focused on developing novel modalities for silent speech interfaces (SSI).

During Spring 2025, I have had an exchange semester at <a href="https://www.cornell.edu/" style="text-decoration: none;">Cornell University</a>, where I join <a href="https://www.scifilab.org/" style="text-decoration: none;">SciFi Lab</a> supervised by <a href="https://czhang.org/" style="text-decoration: none;">Prof. Cheng Zhang</a>. Our research focused on predicting applied grip force from skin deformation using active acoustic sensing on a wristband.

During Summer 2025, I was working on PMUTs-based intelligent interface in <a href="https://me.berkeley.edu/laboratories/lin-lab/" style="text-decoration: none;">Lin Lab</a> at <a href="https://www.berkeley.edu/" style="text-decoration: none;">UCBerkeley</a>, under the supervision of <a href="https://lwlin.me.berkeley.edu/" style="text-decoration: none;">Prof. Liwei Lin</a>.

<br>
<br>


# üìñ Educations
<div class="education-box"> 
<div class="education-box-image">
<img src="images/logo/Tsinghua_logo.png" alt="sym" width="100%">
</div>
<div class="education-box-text" markdown="1">
<a href="https://www.tsinghua.edu.cn/en/" style="text-decoration: none;"><strong>Tsinghua University</strong></a>
<p> <strong>B.E. in Electronic Engineering</strong> </p>
<p> GPA: 3.92/4.00 </p>
<p> <i>2022.09 - 2026.06</i> (Expected) </p>
<p> <i>Beijing, China</i> </p>
</div>
</div>

<br>
<br>

<div class="education-box"> 
<div class="education-box-image">
<img src="images/logo/Cornell_logo.png" alt="sym" width="100%">
</div>
<div class="education-box-text" markdown="1">
<a href="https://www.cornell.edu/" style="text-decoration: none;"><strong>Cornell University</strong></a>
<p> <strong>Exchange Student</strong> </p>
<p> GPA: 4.30/4.30 </p>
<p> <i>2025.01 - 2025.05</i></p>
<p> <i>Ithaca, New York, United States</i> </p>
</div>
</div>

<br>
<br>


# üî¨ Research Experiences

<div class="education-box"> 
<div class="education-box-image">
<img src="images/logo/Berkeley_logo.png" alt="sym" width="100%">
</div>
<div class="education-box-text" markdown="1">
<a href="https://linlab.me.berkeley.edu/" style="text-decoration: none;"><strong>Lin Lab</strong></a>, 
<a href="https://www.berkeley.edu/" style="text-decoration: none;"><strong>University of California, Berkeley</strong></a>
<p> Supervisor: <a href="https://lwlin.me.berkeley.edu/" style="text-decoration: none;"><strong>Prof. Liwei Lin</strong></a> </p>
<p> Research Topics: <strong>Piezoelectric Micromachined Ultrasonic Transducers (PMUTs)</strong> </p>
<p> <i>2025.05 - 2025.09</i> </p>
</div>
</div>

<br>
<br>

<div class="education-box"> 
<div class="education-box-image">
<img src="images/logo/Cornell_logo.png" alt="sym" width="100%">
</div>
<div class="education-box-text" markdown="1">
<a href="https://www.scifilab.org/" style="text-decoration: none;"><strong>SciFi Lab</strong></a>, 
<a href="https://www.cornell.edu/" style="text-decoration: none;"><strong>Cornell University</strong></a>
<p> Supervisor: <a href="https://czhang.org/" style="text-decoration: none;"><strong>Prof. Cheng Zhang</strong></a> </p>
<p> Research Topics: <strong>Ultrasonic Sensing, Active Acoustic Sensing</strong></p>
<p> <i>2025.01 - 2025.06</i> </p>
</div>
</div>

<br>
<br>

<div class="education-box"> 
<div class="education-box-image">
<img src="images/logo/Tsinghua_logo.png" alt="sym" width="100%">
</div>
<div class="education-box-text" markdown="1">
<a href="https://www.sic.tsinghua.edu.cn/en/" style="text-decoration: none;"><strong>School of Integrated Circuit</strong></a>, 
<a href="https://www.tsinghua.edu.cn/en/" style="text-decoration: none;"><strong>Tsinghua University</strong></a>
<p> Supervisor: <a href="https://www.sic.tsinghua.edu.cn/en/info/1078/1373.htm" style="text-decoration: none;"><strong>Prof. Tianling Ren</strong></a> </p>
<p> Research Topics: <strong>Silent Speech Interfaces, Haptics</strong></p>
<p> <i>2023.09 - 2024.11</i> </p>
</div>
</div>

<br>
<br>


# üìù Publications 
<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">IEEE MEMS 2026</div>
      <img src='images/publications/PMUTButton_MEMS_20251020.png' alt="sym" width="100%">
      <!-- <video src="images/publications/objnerf2.mp4" width="100%" autoplay muted loop></video> -->
    </div>
  </div>

  <div class='paper-box-text' markdown="1">

  <!-- <a href="https://objnerf.github.io" style="text-decoration: none;"><strong>Obj-NeRF: Extracting Object NeRFs from Multi-view Images</strong></a> -->
  <a href="files/PMUTButton_MEMS_20251020.pdf"  style="text-decoration: none;"><strong>AN ACOUSTIC TOUCH-MOTION BUTTON WITH HAPTIC FUNCTION VIA AN IN-SITU FABRICATED ELASTOMERIC LENS ATOP PMUTS</strong></a>

  <p> Declan Fitzgerald*, <strong>Yudong Xie*</strong>, Sean Isomatsu, Nikita Lukhanin, Zihan Wang, Liwei Lin </p>
  <p> *<i>Co-first Authors</i> </p>

  <p><i>The 39th International Conference on Micro Electro Mechanical Systems (IEEE MEMS 2026)</i></p>

  <a href="files/PMUTButton_MEMS_20251020.pdf"> <strong>Abstract</strong> </a>
  <!-- <a href="https://objnerf.github.io" ><strong>Project</strong></a>,  -->
  <!-- <a href="https://arxiv.org/abs/2311.15291" ><strong>Paper</strong></a>,  -->
  <!-- <a href=""><strong>Code</strong></a>,  -->
  <!-- <a href="https://www.youtube.com/watch?v=VEwYrSPFatg"><strong>Video</strong></a>, 
  <a href="sources/paper_objnerf/paper.pdf"><strong>PDF</strong></a> -->
  <!-- <a href=""><strong>BibTex</strong></a>  -->
  <!-- <a href=""><strong>Supp</strong></a>,  -->
  <!-- <a href=""><strong>Poster</strong></a>,  -->
  <!-- <a href=""><strong>Slides</strong></a> -->
  <!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

  <!-- - Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

  <details class="abstract-details">
    <summary class="abstract-summary">
      <span class="triangle">‚ñ∂</span>
      <span class="summary-text">Brief Introduction</span>
    </summary>
    <div class="abstract-content">
      Here we present an integrated manufacturing process and implementation of a variable acoustic touch motion button with haptic function via in-situ fabricated elastomeric lens atop piezoelectric micromachined ultrasonic transducers (PMUTs). Three distinctive features have been achieved as compared to the state-of-art: (1) direct integration of an elastomeric lens with an acoustic pressure focus point of larger than 3000 Pa for enhanced haptic feedback sensation; (2) pressure-sensitive recognitions with variable levels for touch motions; and (3) ML (machine learning)-based regression with adaptive bidirectional interface for high sensing accuracy. As such, this work represents a new class of human-machine interface with scalable ultrasonic MEMS hardware and intelligent software for touch-sensation applications in hand-held device buttons, including cell phones.
    </div>
  </details>

  </div>
</div>

<br>

<div class='paper-box'> 

  <div class='paper-box-image'>
    <div>
      <div class="badge">ISWC '25</div>
      <img src='images/publications/Echoforce_ISWC_20250717.png' alt="sym" width="100%">
    </div>
  </div>

  <div class='paper-box-text' markdown="1">
  <a href="https://dl.acm.org/doi/10.1145/3715071.3750405"  style="text-decoration: none;"><strong>EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband</strong></a>

  <p>Kian Mahmoodi*, <strong>Yudong Xie*</strong>, Tan Gemicioglu*, Chi-Jung Lee, Jiwan Kim, Cheng Zhang</p>
  <p> *<i>Co-first Authors</i> </p>
  <p><i>In Proceedings of the 2025 ACM International Symposium on Wearable Computers (ISWC ‚Äô25)</i></p>

  <a href="https://dl.acm.org/doi/pdf/10.1145/3715071.3750405"><strong>Paper</strong></a>
  <!-- <a href="../sources/paper_iwcmc/codes.zip" download><strong>Code</strong></a>,  -->
  <!-- <a href="../sources/paper_iwcmc/slides.pdf" download><strong>Slides</strong></a>,  -->
  <!-- <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:4uNrrGSRwJgJ:scholar.google.com/&output=citation&scisdr=ClFyCqlnEM2dlci8V2k:AFWwaeYAAAAAZWK6T2nnKfpSbHf8SzuCl9ZIghU&scisig=AFWwaeYAAAAAZWK6T12eM1unCqehCl_RtPFF6Qc&scisf=4&ct=citation&cd=-1&hl=en"><strong>BibTex</strong></a>  -->

  <details class="abstract-details">
    <summary class="abstract-summary">
      <span class="triangle">‚ñ∂</span>
      <span class="summary-text">Abstract</span>
    </summary>
    <div class="abstract-content">
      Grip force is commonly used as an overall health indicator in older adults and is valuable for tracking progress in physical training and rehabilitation. Existing methods for wearable grip force measurement are cumbersome and user-dependent, making them insufficient for practical, continuous grip force measurement. We introduce EchoForce, a novel wristband using acoustic sensing for low-cost, non-contact measurement of grip force. EchoForce captures acoustic signals reflected from subtle skin deformations by f lexor muscles on the forearm. In a user study with 11 participants, EchoForce achieved a fine-tuned user-dependent mean error rate of 9.08% and a user-independent mean error rate of 12.3% using a foundation model. Our system remained accurate between sessions, hand orientations, and users, overcoming a significant limitation of past force sensing systems. EchoForce makes continuous grip force measurement practical, providing an effective tool for health monitoring and novel interaction techniques.
    </div>
  </details>

  </div>
</div>

<br>

<div class='paper-box'> 

  <div class='paper-box-image'>
    <div>
      <div class="badge">In Submission</div>
      <img src='images/publications/SilentSpeech_Arxiv_20250920.png' alt="sym" width="100%">
    </div>
  </div>

  <div class='paper-box-text' markdown="1">
  <a href="https://doi.org/10.48550/arXiv.2502.17829"  style="text-decoration: none;"><strong>Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm</strong></a>

  <p><strong>Yudong Xie</strong>, Zhifeng Han, Qinfan Xiao, Liwei Liang, Luqi Tao, Tianling Ren</p>
  <!-- <p><i>In Proceedings of the 2025 ACM International Symposium on Wearable Computers (ISWC ‚Äô25)</i></p> -->

  <a href="https://arxiv.org/pdf/2502.17829"><strong>Preprint</strong></a>

  <details class="abstract-details">
    <summary class="abstract-summary">
      <span class="triangle">‚ñ∂</span>
      <span class="summary-text">Abstract</span>
    </summary>
    <div class="abstract-content">
      Silent speech interfaces (SSI) are being actively developed to assist individuals with communication impairments who have long suffered from a reduced quality of life. However, silent sentences are difficult to segment and recognize due to elision and linking. A novel silent speech sentence recognition method is proposed to convert the facial motion signals collected by six-axis accelerometers into transcribed words and sentences. A Conformer-based neural network with the Connectionist-Temporal-Classification algorithm gains contextual understanding and translates the non-acoustic signals into words sequences. Test results show that the proposed method achieves a 97.17% accuracy in sentence recognition, surpassing the existing silent speech recognition methods with a typical accuracy of 85%-95%, and demonstrating the potential of accelerometers as an available SSI modality for high-accuracy silent speech sentence recognition.
    </div>
  </details>

  </div>
</div>

<!-- - Jida Zhang, <strong>Zhiyi Li</strong>, Zijian Zhang, <a href="https://ieeexplore.ieee.org/abstract/document/10122216/" style="text-decoration: none;">Wideband Active RISs: Architecture, Modeling, and Beamforming Design</a>, <i>IEEE Communications Letters</i>, vol. 27, pp. 1899-1903, Sep. 2023. 

---

- <strong>Zhiyi Li</strong>, Jida Zhang, Jieao Zhu, Linglong Dai, <a href="https://arxiv.org/abs/2310.15901" style="text-decoration: none;">Enhancing Energy Efficiency for Reconfigurable Intelligent Surfaces with Practical Power Models</a>, <i>arXiv preprint arXiv:2310.15901</i> (2023).  -->

<br>
<br>

# üèÖ Honors and Awards

## Scholarships
- *2025.10* &nbsp; "December 9th" Scholarship <i>(Top 2% in Tsinghua)</i> (~$2800)
- *2024.10* &nbsp; National Scholarship <i>(9/263)</i> (~$1400)
- *2024.10* &nbsp; Tsinghua Alumni Zhihua Integrated Circuit Scholarship (~$1400)
- *2023.10* &nbsp; National Scholarship <i>(4/261)</i> (~$1400)

## Awards
- *2024.02* &nbsp; Meritorious Winner of 2024 Mathematical Contest in Modeling
- *2023.12* &nbsp; First Prize in The 39th National Regional College Student Physics Competition
- *2023.09* &nbsp; Champion of 2023 Ricoh Hackathon (Beijing, Tianjin and Hebei) (~$4000)

## Sports
- *2024.05* &nbsp; Champion of 2024 Beijing College Students Volleyball League (Division B)
- *2024.04* &nbsp; Champion of 2024 Tsinghua University John Mo Cup (Men‚Äôs Volleyball)
- *2023.04* &nbsp; Second place of 2023 Tsinghua University John Mo Cup (Men‚Äôs Volleyball)

<br>
<br>

# üìë CV

<div style="text-align: center; margin: 30px 0;">
    <a href="/files/CV.pdf" download class="btn" style="background: #00369f; color: white; padding: 12px 24px; text-decoration: none; border-radius: 5px; font-weight: bold; display: inline-block;">
        üì• Download My CV (PDF)
    </a>
</div>

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<!-- # üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<div>
<p><i>Last Updated: Nov. 6, 2025</i></p>
</div>